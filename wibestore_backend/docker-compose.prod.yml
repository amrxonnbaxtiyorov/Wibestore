version: "3.9"

services:
  web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: wibestore-web
    restart: always
    env_file:
      - .env
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
    volumes:
      - static_volume:/app/staticfiles
      - media_volume:/app/media
      - logs_volume:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: >
      gunicorn config.wsgi:application --bind 0.0.0.0:8000 --workers 4 --worker-class gthread --threads 4 --timeout 120 --access-logfile /app/logs/gunicorn-access.log --error-logfile /app/logs/gunicorn-error.log
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health/" ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - wibestore

  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: wibestore-celery-worker
    restart: always
    env_file:
      - .env
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: >
      celery -A config worker --loglevel=info --concurrency=4 --max-tasks-per-child=1000 --logfile=/app/logs/celery-worker.log
    volumes:
      - logs_volume:/app/logs
    networks:
      - wibestore

  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: wibestore-celery-beat
    restart: always
    env_file:
      - .env
    environment:
      - DJANGO_SETTINGS_MODULE=config.settings.production
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: >
      celery -A config beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler --logfile=/app/logs/celery-beat.log
    volumes:
      - logs_volume:/app/logs
    networks:
      - wibestore

  postgres:
    image: postgres:16-alpine
    container_name: wibestore-postgres
    restart: always
    environment:
      POSTGRES_DB: ${DB_NAME:-wibestore_db}
      POSTGRES_USER: ${DB_USER:-wibestore}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-wibestore_password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${DB_USER:-wibestore} -d ${DB_NAME:-wibestore_db}" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - wibestore

  redis:
    image: redis:7-alpine
    container_name: wibestore-redis
    restart: always
    command: >
      redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - wibestore

  nginx:
    image: nginx:1.25-alpine
    container_name: wibestore-nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - static_volume:/var/www/static:ro
      - media_volume:/var/www/media:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - web
    networks:
      - wibestore

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  static_volume:
    driver: local
  media_volume:
    driver: local
  logs_volume:
    driver: local

networks:
  wibestore:
    driver: bridge
